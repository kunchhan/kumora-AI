{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Set Fro Mensturation Based Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Label columns: ['Improved mood', 'Hopefulness', 'Renewed energy', 'Optimism', 'Productivity', 'Clarity', 'Confidence', 'High energy', 'Sociability', 'Empowerment', 'Motivation', 'Sadness', 'Tearfulness', 'Low self-esteem', 'Loneliness or Isolation', 'Feeling overwhelmed', 'Anger or frustration', 'Irritability', 'Mood swings', 'Anxiety', 'Sensitivity to rejection', 'Restlessness', 'Emotional sensitivity', 'Physical discomfort', 'Attractiveness', 'Sexual drive', 'Feeling in control', 'Gratitude', 'Relief']\n",
      "Train size: 4500\n",
      "Validation size: 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import ast\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../../dataset/menstrual_emotion/synthetic_data_womens_health.csv\")  # Update path as needed\n",
    "# Convert stringified emotion dictionaries into real dicts (if needed)\n",
    "if isinstance(df[\"emotions\"].iloc[0], str):\n",
    "    df[\"emotions\"] = df[\"emotions\"].apply(ast.literal_eval)\n",
    "\n",
    "# Expand the emotions column into multiple columns\n",
    "emotion_df = df[\"emotions\"].apply(pd.Series)\n",
    "\n",
    "# Combine with the text column\n",
    "df_ready = pd.concat([df[\"text\"], emotion_df], axis=1)\n",
    "\n",
    "# Replace NaNs with 0\n",
    "df_ready.fillna(0, inplace=True)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df_ready)\n",
    "\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset_raw = dataset[\"train\"]\n",
    "val_dataset_raw = dataset[\"test\"]\n",
    "\n",
    "label_columns = list(emotion_df.columns)\n",
    "print(f\"Label columns: {label_columns}\")\n",
    "\n",
    "print(f\"Train size: {len(train_dataset_raw)}\")\n",
    "print(f\"Validation size: {len(val_dataset_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Pereparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Starting this new job has me feeling anxious but alquite hopeful. It's scary and exciting at the same time.\", 'Improved mood': 0.0, 'Hopefulness': 0.0, 'Renewed energy': 0.0, 'Optimism': 1.0, 'Productivity': 0.0, 'Clarity': 1.0, 'Confidence': 1.0, 'High energy': 0.0, 'Sociability': 0.0, 'Empowerment': 0.0, 'Motivation': 1.0, 'Sadness': 0.0, 'Tearfulness': 0.0, 'Low self-esteem': 0.0, 'Loneliness or Isolation': 0.0, 'Feeling overwhelmed': 0.0, 'Anger or frustration': 0.0, 'Irritability': 0.0, 'Mood swings': 0.0, 'Anxiety': 0.0, 'Sensitivity to rejection': 0.0, 'Restlessness': 0.0, 'Emotional sensitivity': 0.0, 'Physical discomfort': 0.0, 'Attractiveness': 1.0, 'Sexual drive': 0.0, 'Feeling in control': 1.0, 'Gratitude': 0.0, 'Relief': 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_dataset_raw[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2d37929ddd4ff2bdbbc67e5c318e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0072b21d02b248d8b763aaff68778e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "num_labels = len(label_columns)\n",
    "\n",
    "def preprocess(example):\n",
    "    return {\n",
    "        \"text\": example[\"text\"],\n",
    "        \"labels\": [example[label] for label in label_columns]\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset_raw.map(preprocess)\n",
    "val_dataset = val_dataset_raw.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5f7b4195f24b289acb01cf731a7d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e713fbf91e864e45ad6c48a7c4b43e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered train size: 4500\n",
      "Filtered validation size: 500\n"
     ]
    }
   ],
   "source": [
    "# Filter Neutral Samples as empty samples add noise and teach the model that predicting nothing is normal\n",
    "# Filter training samples that have at least one label\n",
    "train_dataset = train_dataset.filter(lambda x: sum(x['labels']) > 0)\n",
    "val_dataset = val_dataset.filter(lambda x: sum(x['labels']) > 0)\n",
    "\n",
    "print(f\"Filtered train size: {len(train_dataset)}\")\n",
    "print(f\"Filtered validation size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average labels/sample: 5.2651111111111115\n",
      "Unique label values: [0. 1.]\n",
      "\n",
      "\n",
      "Unique label vectors in training set: 2626\n",
      "Train sample 0: Feeling incredibly confident and attractive today. Is this what they mean by ovulation glow?\n",
      "Val sample 0:   Took a mental health day and already experiencing more in control. Sometimes you just need to pause.\n",
      "Train sample 1: Starting this new job has me feeling anxious but alquite hopeful. It's scary and exciting at the same time.\n",
      "Val sample 1:   Started exercising again and the endorphins are real! Feeling motivated and strong.\n",
      "Train sample 2: My best friend just gets me. After our talk, I feel supported and understood.\n",
      "Val sample 2:   Becoming a mom has brought so many emotions - joy, fear, overwhelming love, and complete exhaustion.\n",
      "Train sample 3: Had a fight with my partner and feel so hurt and misunderstood. Why is communication so hard?\n",
      "Val sample 3:   Took a mental health day and already feeling more in control. Sometimes you just need to pause.\n",
      "Train sample 4: Dating in my 30s is exhausting. Feeling lonely but trying to stay hopeful.\n",
      "Val sample 4:   My energy is through the roof and I feel so social. Called three friends just to chat!\n"
     ]
    }
   ],
   "source": [
    "label_counts = [sum(example['labels']) for example in train_dataset]\n",
    "print(\"Average labels/sample:\", np.mean(label_counts))\n",
    "print(\"Unique label values:\", np.unique(train_dataset[0]['labels']))\n",
    "\n",
    "\n",
    "labels_array = np.array([x['labels'] for x in train_dataset])\n",
    "print(\"\\n\\nUnique label vectors in training set:\", np.unique(labels_array, axis=0).shape[0])\n",
    "for i in range(5):\n",
    "    print(f\"Train sample {i}: {train_dataset[i]['text']}\")\n",
    "    print(f\"Val sample {i}:   {val_dataset[i]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tokenize the Text with DistilBertTokenizer Fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fa8736d6764143926d10db150f941b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de08b081a3e40dfb35d9a9267fc8094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of tokenized train dataset:\n",
      "{'labels': tensor([0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.]), 'input_ids': tensor([  101,  3225,  2023,  2047,  3105,  2038,  2033,  3110, 11480,  2021,\n",
      "         2632, 15549,  2618, 17772,  1012,  2009,  1005,  1055, 12459,  1998,\n",
      "        10990,  2012,  1996,  2168,  2051,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset_tk = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset_tk = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# This ensures your dataset is ready for PyTorch training\n",
    "train_dataset_tk.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset_tk.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "print(\"Head of tokenized train dataset:\")\n",
    "print(train_dataset_tk[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "labels_matrix = np.array(train_dataset[\"labels\"])\n",
    "label_freq = labels_matrix.sum(axis=0)\n",
    "num_samples = labels_matrix.shape[0]\n",
    "\n",
    "# Avoid divide-by-zero and clip very large weights\n",
    "pos_weights = (num_samples - label_freq) / (label_freq + 1e-5)\n",
    "pos_weights = np.clip(pos_weights, a_min=1.0, a_max=None)\n",
    "\n",
    "class_weights_tensor = torch.tensor(pos_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "    probs = sigmoid(preds)\n",
    "    optimal_threshold = 0.7  # You can try different values later\n",
    "\n",
    "    y_pred = np.where(probs >= optimal_threshold, 1, 0)\n",
    "\n",
    "    print(\"Sample true:\", labels[0])\n",
    "    print(\"Sample pred:\", y_pred[0])\n",
    "    # print(\"Raw logits sample:\", preds[0])\n",
    "    # print(\"Sigmoid probs sample:\", probs[0])\n",
    "    \n",
    "    f1 = f1_score(labels, y_pred, average='micro')\n",
    "    acc = accuracy_score(labels, y_pred)\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ").to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lotuso\",\n",
    "    eval_strategy=\"epoch\",   \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "class BestF1Callback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.best_f1 = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        f1 = metrics.get(\"eval_f1\", 0)\n",
    "        if f1 > self.best_f1:\n",
    "            print(f\"\\nNew best F1: {f1:.4f}\")\n",
    "            self.best_f1 = f1\n",
    "            control.should_save = True\n",
    "        else:\n",
    "            control.should_save = False\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 16  # Number of epochs\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset_tk, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset_tk, batch_size=16,num_workers=0)\n",
    "\n",
    "def train_model(train_loader, val_loader, model, training_args):\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device).float()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} - Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "                labels = batch[\"labels\"].cpu().numpy()\n",
    "                logits = model(**inputs).logits\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels)\n",
    "\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"micro\")\n",
    "        print(f\"Epoch {epoch + 1} - Validation F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 282/282 [00:43<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.4103\n",
      "Epoch 1 - Validation F1: 0.2672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 282/282 [00:43<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 0.4101\n",
      "Epoch 2 - Validation F1: 0.2669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 282/282 [00:44<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 0.4097\n",
      "Epoch 3 - Validation F1: 0.2732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 282/282 [00:43<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training Loss: 0.4096\n",
      "Epoch 4 - Validation F1: 0.2698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 282/282 [00:42<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training Loss: 0.4094\n",
      "Epoch 5 - Validation F1: 0.2670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 282/282 [00:43<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training Loss: 0.4095\n",
      "Epoch 6 - Validation F1: 0.2703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 282/282 [00:42<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training Loss: 0.4088\n",
      "Epoch 7 - Validation F1: 0.2720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 282/282 [00:44<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training Loss: 0.4090\n",
      "Epoch 8 - Validation F1: 0.2726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 282/282 [00:45<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training Loss: 0.4083\n",
      "Epoch 9 - Validation F1: 0.2714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 282/282 [00:46<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training Loss: 0.4086\n",
      "Epoch 10 - Validation F1: 0.2687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 282/282 [00:46<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Training Loss: 0.4083\n",
      "Epoch 11 - Validation F1: 0.2685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 282/282 [00:47<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Training Loss: 0.4074\n",
      "Epoch 12 - Validation F1: 0.2763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 282/282 [00:47<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Training Loss: 0.4076\n",
      "Epoch 13 - Validation F1: 0.2732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 282/282 [00:47<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Training Loss: 0.4067\n",
      "Epoch 14 - Validation F1: 0.2782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 282/282 [00:47<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Training Loss: 0.4064\n",
      "Epoch 15 - Validation F1: 0.2665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 282/282 [00:47<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Training Loss: 0.4066\n",
      "Epoch 16 - Validation F1: 0.2656\n"
     ]
    }
   ],
   "source": [
    "hist = train_model(train_loader, val_loader, model, training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), '../../models/lotus_menstrual_emotion/lotus_menstrual_emotion_classifier_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../models/lotus_menstrual_emotion/lotus_menstrual_emotion_model_v1/tokenizer_config.json',\n",
       " '../../models/lotus_menstrual_emotion/lotus_menstrual_emotion_model_v1/special_tokens_map.json',\n",
       " '../../models/lotus_menstrual_emotion/lotus_menstrual_emotion_model_v1/vocab.txt',\n",
       " '../../models/lotus_menstrual_emotion/lotus_menstrual_emotion_model_v1/added_tokens.json',\n",
       " '../../models/lotus_menstrual_emotion/lotus_menstrual_emotion_model_v1/tokenizer.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# You already have the emotion label columns as list\n",
    "label_list = label_columns  # this comes from earlier step where we extracted all label column names\n",
    "\n",
    "# Build mapping (just use identity mappings for now)\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# Update model config\n",
    "model.config.id2label = {str(k): v for k, v in id2label.items()}\n",
    "model.config.label2id = label2id\n",
    "model.config.num_labels = len(label_list)\n",
    "model.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "\n",
    "# Save model and tokenizer huggingface format\n",
    "model.save_pretrained(\"../../models/lotus_menstrual_emotion/lotus_menstrual_emotion_model_v1\")\n",
    "tokenizer.save_pretrained(\"../../models/lotus_menstrual_emotion/lotus_menstrual_emotion_model_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Feeling in control', 0.6071305274963379)]\n",
      "[('Feeling overwhelmed', 0.5021209120750427)]\n",
      "[('Tearfulness', 0.6932561993598938), ('Low self-esteem', 0.6783528923988342), ('Feeling overwhelmed', 0.6270961165428162), ('Physical discomfort', 0.6934010982513428), ('Relief', 0.7682949304580688)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))  # multi-label output\n",
    "\n",
    "# You already defined:\n",
    "# tokenizer, model, device, emotion_columns\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_columns)}\n",
    "\n",
    "def predict_emotions(text, threshold=0.5):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = sigmoid(logits.cpu().numpy()[0])\n",
    "\n",
    "    return [(id2label[i], float(p)) for i, p in enumerate(probs) if p >= threshold]\n",
    "\n",
    "\n",
    "# Example\n",
    "#print(predict_emotions(\"I am scared and angry, but also a bit hopeful.\"))\n",
    "\n",
    "print(predict_emotions(\"I am so happy today! The sun is shining and I feel great.\"))\n",
    "print(predict_emotions(\"I am in great pain\"))\n",
    "print(predict_emotions(\"I never ever got cramps, ever. Just some acne. And then one time when I was...21? 22? I wound up spending day 1 vomiting and ever since I've gotten 2-3 days of horrible, horrible cramps.\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
